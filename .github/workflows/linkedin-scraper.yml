name: LinkedIn Salary Data Collection

on:
  # Run daily at 1 AM UTC (before ML pipeline at 2 AM)
  schedule:
    - cron: '0 1 * * *'

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      runners:
        description: 'Number of parallel runners (1-10)'
        type: choice
        options:
          - '10'
          - '5'
        default: '10'

permissions:
  id-token: write
  contents: read

jobs:
  # Job 1: Generate search query matrix
  generate-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      timestamp: ${{ steps.set-matrix.outputs.timestamp }}

    steps:
      - name: Generate search matrix
        id: set-matrix
        run: |
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H-%M-%SZ")
          echo "timestamp=$TIMESTAMP" >> $GITHUB_OUTPUT

          # Create matrix with search query batches
          # All searches use salary: "50000" to match model's minimum
          # Diversified by role level and specialization instead of salary
          cat << 'EOF' > matrix.json
          {
            "include": [
              {
                "batch": "1",
                "queries": [
                  {"keyword": "junior machine learning engineer", "location": "United States", "salary": "50000", "limit": "100"},
                  {"keyword": "entry level data scientist", "location": "United States", "salary": "50000", "limit": "100"}
                ]
              },
              {
                "batch": "2",
                "queries": [
                  {"keyword": "machine learning engineer", "location": "United States", "salary": "50000", "limit": "100"},
                  {"keyword": "data scientist", "location": "United States", "salary": "50000", "limit": "100"}
                ]
              },
              {
                "batch": "3",
                "queries": [
                  {"keyword": "senior machine learning engineer", "location": "United States", "salary": "50000", "limit": "100"},
                  {"keyword": "senior data scientist", "location": "United States", "salary": "50000", "limit": "100"}
                ]
              },
              {
                "batch": "4",
                "queries": [
                  {"keyword": "principal machine learning engineer", "location": "United States", "salary": "50000", "limit": "100"},
                  {"keyword": "staff machine learning engineer", "location": "United States", "salary": "50000", "limit": "100"}
                ]
              },
              {
                "batch": "5",
                "queries": [
                  {"keyword": "AI engineer", "location": "United States", "salary": "50000", "limit": "100"},
                  {"keyword": "deep learning engineer", "location": "United States", "salary": "50000", "limit": "100"}
                ]
              },
              {
                "batch": "6",
                "queries": [
                  {"keyword": "computer vision engineer", "location": "United States", "salary": "50000", "limit": "100"},
                  {"keyword": "NLP engineer", "location": "United States", "salary": "50000", "limit": "100"}
                ]
              },
              {
                "batch": "7",
                "queries": [
                  {"keyword": "MLOps engineer", "location": "United States", "salary": "50000", "limit": "100"},
                  {"keyword": "research scientist machine learning", "location": "United States", "salary": "50000", "limit": "100"}
                ]
              },
              {
                "batch": "8",
                "queries": [
                  {"keyword": "machine learning engineer", "location": "California", "salary": "50000", "limit": "100"},
                  {"keyword": "data scientist", "location": "New York", "salary": "50000", "limit": "100"}
                ]
              },
              {
                "batch": "9",
                "queries": [
                  {"keyword": "machine learning engineer", "location": "Washington", "salary": "50000", "limit": "100"},
                  {"keyword": "data scientist", "location": "Massachusetts", "salary": "50000", "limit": "100"}
                ]
              },
              {
                "batch": "10",
                "queries": [
                  {"keyword": "machine learning engineer", "location": "Texas", "salary": "50000", "limit": "100"},
                  {"keyword": "applied scientist", "location": "United States", "salary": "50000", "limit": "100"}
                ]
              }
            ]
          }
          EOF

          echo "matrix=$(cat matrix.json | jq -c .)" >> $GITHUB_OUTPUT

  # Job 2: Parallel data collection
  collect-data:
    needs: generate-matrix
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 10
      fail-fast: false
      matrix: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: linkedin-scraper/package.json

      - name: Install dependencies
        working-directory: linkedin-scraper
        run: npm ci

      - name: Collect data for batch ${{ matrix.batch }}
        working-directory: linkedin-scraper
        run: |
          # Create custom collection script for this batch
          cat << 'EOF' > scripts/collect-batch.js
          const fs = require("fs");
          const path = require("path");
          const linkedIn = require("../index");

          const OUTPUT_DIR = "./data";
          const BATCH_ID = process.env.BATCH_ID;
          const TIMESTAMP = process.env.TIMESTAMP;
          const OUTPUT_FILE = path.join(OUTPUT_DIR, `batch-${BATCH_ID}-${TIMESTAMP}.jsonl`);

          if (!fs.existsSync(OUTPUT_DIR)) {
            fs.mkdirSync(OUTPUT_DIR, { recursive: true });
          }

          const writeStream = fs.createWriteStream(OUTPUT_FILE, { flags: "a" });
          const queries = JSON.parse(process.env.QUERIES);
          const seenJobUrls = new Set();

          let totalJobs = 0;
          let jobsWithSalary = 0;

          function saveJob(job) {
            if (seenJobUrls.has(job.jobUrl)) return false;
            seenJobUrls.add(job.jobUrl);

            const jobData = {
              collected_at: new Date().toISOString(),
              position: job.position,
              company: job.company,
              location: job.location,
              posted_date: job.date,
              job_url: job.jobUrl,
              salary: job.salary,
              seniority_level: job.seniorityLevel,
              employment_type: job.employmentType,
              job_function: job.jobFunction,
              industries: job.industries,
              applicant_count: job.applicantCount,
              experience_years: job.experienceYears,
              education: job.education,
              skills: job.skills || [],
              company_logo: job.companyLogo,
              description: job.description ? job.description.substring(0, 500) : "",
            };

            writeStream.write(JSON.stringify(jobData) + "\n");
            totalJobs++;
            if (job.salary && job.salary !== "Not specified") jobsWithSalary++;
            return true;
          }

          async function collect() {
            console.log(`Batch ${BATCH_ID}: Processing ${queries.length} queries\n`);

            for (const [i, query] of queries.entries()) {
              console.log(`[${i+1}/${queries.length}] ${query.keyword} in ${query.location}`);

              try {
                const jobs = await linkedIn.query({
                  ...query,
                  dateSincePosted: "past Week",
                  sortBy: "recent",
                  fetchJobDetails: true,
                });

                let saved = 0;
                jobs.forEach(job => { if (saveJob(job)) saved++; });
                console.log(`  Saved ${saved} unique jobs`);

                if (i < queries.length - 1) {
                  await new Promise(r => setTimeout(r, 3000));
                }
              } catch (error) {
                console.error(`  Error: ${error.message}`);
              }
            }

            writeStream.end();
            console.log(`\nBatch ${BATCH_ID} complete: ${totalJobs} jobs (${jobsWithSalary} with salary)`);
          }

          collect().catch(e => { console.error(e); process.exit(1); });
          EOF

          # Debug: Show what we're about to run
          echo "=== Starting batch collection ==="
          echo "Batch ID: ${{ matrix.batch }}"
          echo "Timestamp: ${{ needs.generate-matrix.outputs.timestamp }}"
          echo "Queries: ${{ toJson(matrix.queries) }}"
          echo "=================================="

          # Run the batch collection
          BATCH_ID=${{ matrix.batch }} \
          TIMESTAMP=${{ needs.generate-matrix.outputs.timestamp }} \
          QUERIES='${{ toJson(matrix.queries) }}' \
          node scripts/collect-batch.js

          # Verify the output file was created
          EXPECTED_FILE="./data/batch-${{ matrix.batch }}-${{ needs.generate-matrix.outputs.timestamp }}.jsonl"
          if [ -f "$EXPECTED_FILE" ]; then
            LINE_COUNT=$(wc -l < "$EXPECTED_FILE")
            echo "✓ Created $EXPECTED_FILE ($LINE_COUNT jobs)"
          else
            echo "✗ ERROR: Expected file not created: $EXPECTED_FILE"
            exit 1
          fi

        env:
          NODE_ENV: production

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Upload batch data to S3
        working-directory: linkedin-scraper
        run: |
          # Verify batch file exists
          BATCH_FILE=$(ls -1 ./data/batch-${{ matrix.batch }}-*.jsonl 2>/dev/null || echo "")

          if [ -z "$BATCH_FILE" ]; then
            echo "ERROR: No batch file found for batch ${{ matrix.batch }}"
            echo "Files in ./data/:"
            ls -la ./data/ || echo "No data directory"
            exit 1
          fi

          echo "Uploading: $BATCH_FILE"
          FILE_SIZE=$(du -h "$BATCH_FILE" | cut -f1)
          echo "File size: $FILE_SIZE"

          # Upload to S3
          aws s3 sync ./data s3://ai-salary-predictor/data/linkedin/raw/ \
            --exclude "*" \
            --include "batch-${{ matrix.batch }}-*.jsonl" \
            --storage-class INTELLIGENT_TIERING

          echo "✓ Uploaded batch ${{ matrix.batch }} to S3 ($FILE_SIZE)"

  # Job 3: Consolidate and deduplicate
  consolidate:
    needs: [generate-matrix, collect-data]
    if: success()  # Only run if all collect-data jobs succeeded
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: linkedin-scraper/package.json

      - name: Install dependencies
        working-directory: linkedin-scraper
        run: npm ci

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Download all batch files from S3
        working-directory: linkedin-scraper
        run: |
          mkdir -p ./data

          # Debug: List files on S3 for this timestamp
          echo "Looking for files with timestamp: ${{ needs.generate-matrix.outputs.timestamp }}"
          echo "Files on S3 matching pattern:"
          aws s3 ls s3://ai-salary-predictor/data/linkedin/raw/ | grep "batch-.*-${{ needs.generate-matrix.outputs.timestamp }}.jsonl" || echo "No matching files found"

          # Download batch files
          aws s3 sync s3://ai-salary-predictor/data/linkedin/raw/ ./data/ \
            --exclude "*" \
            --include "batch-*-${{ needs.generate-matrix.outputs.timestamp }}.jsonl"

          # Verify files were downloaded
          FILE_COUNT=$(ls -1 ./data/batch-*.jsonl 2>/dev/null | wc -l)
          echo "Downloaded $FILE_COUNT batch files"

          if [ "$FILE_COUNT" -eq 0 ]; then
            echo "ERROR: No batch files downloaded from S3"
            echo "Checking all files in linkedin/raw/:"
            aws s3 ls s3://ai-salary-predictor/data/linkedin/raw/ --recursive | tail -20
            exit 1
          fi

      - name: Deduplicate consolidated data
        working-directory: linkedin-scraper
        run: |
          node scripts/deduplicate-data.js \
            ./data \
            ./data/consolidated-${{ needs.generate-matrix.outputs.timestamp }}.jsonl

      - name: Upload consolidated file
        working-directory: linkedin-scraper
        run: |
          aws s3 cp \
            ./data/consolidated-${{ needs.generate-matrix.outputs.timestamp }}.jsonl \
            s3://ai-salary-predictor/data/linkedin/processed/ \
            --storage-class INTELLIGENT_TIERING

      # Comprehensive consolidation of ALL LinkedIn data
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: pip install pandas pyarrow boto3

      - name: Download ALL LinkedIn data from S3
        run: |
          mkdir -p data/linkedin/raw data/linkedin/processed

          echo "Downloading all historical batch files..."
          aws s3 sync s3://ai-salary-predictor/data/linkedin/raw/ data/linkedin/raw/ \
            --exclude "*" \
            --include "batch-*.jsonl" \
            --include "ai-jobs-*.jsonl"

          echo "Downloading all consolidated files..."
          aws s3 sync s3://ai-salary-predictor/data/linkedin/processed/ data/linkedin/processed/ \
            --exclude "*" \
            --include "consolidated-*.jsonl"

          echo "Downloaded files:"
          echo "  Raw batches: $(ls -1 data/linkedin/raw/batch-*.jsonl 2>/dev/null | wc -l)"
          echo "  AI jobs: $(ls -1 data/linkedin/raw/ai-jobs-*.jsonl 2>/dev/null | wc -l)"
          echo "  Consolidated: $(ls -1 data/linkedin/processed/consolidated-*.jsonl 2>/dev/null | wc -l)"

      - name: Consolidate ALL LinkedIn data
        run: |
          echo "Running comprehensive consolidation..."
          python3 scripts/consolidate_linkedin_local.py

          # Show final count
          if [ -f "data/linkedin/processed/linkedin_ai_jobs.parquet" ]; then
            TOTAL_RECORDS=$(python3 -c "import pandas as pd; print(len(pd.read_parquet('data/linkedin/processed/linkedin_ai_jobs.parquet')))" 2>/dev/null || echo "0")
            echo "Total consolidated records: $TOTAL_RECORDS"
            echo "total_records=$TOTAL_RECORDS" >> $GITHUB_ENV
          fi

      - name: Upload comprehensive consolidated data
        run: |
          # Upload the comprehensive parquet file
          aws s3 cp data/linkedin/processed/linkedin_ai_jobs.parquet \
            s3://ai-salary-predictor/data/linkedin/processed/linkedin_ai_jobs.parquet \
            --storage-class INTELLIGENT_TIERING

          echo "✓ Uploaded comprehensive consolidated file to S3"

      - name: Create summary
        working-directory: linkedin-scraper
        run: |
          TIMESTAMP="${{ needs.generate-matrix.outputs.timestamp }}"
          TOTAL_SIZE=$(du -sh ./data | cut -f1)
          FILE_COUNT=$(ls -1 ./data/*.jsonl 2>/dev/null | wc -l)

          jq -n \
            --arg ts "$TIMESTAMP" \
            --arg size "$TOTAL_SIZE" \
            --arg files "$FILE_COUNT" \
            '{
              timestamp: $ts,
              total_size: $size,
              files_collected: $files,
              status: "success",
              parallel_runners: 10
            }' > ./data/run-summary.json

          aws s3 cp ./data/run-summary.json \
            s3://ai-salary-predictor/data/linkedin/metadata/latest-run.json

          echo "## LinkedIn Data Collection Complete! ✅" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $TIMESTAMP" >> $GITHUB_STEP_SUMMARY
          echo "**Total Size:** $TOTAL_SIZE" >> $GITHUB_STEP_SUMMARY
          echo "**Files Collected:** $FILE_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat ./data/run-summary.json >> $GITHUB_STEP_SUMMARY
