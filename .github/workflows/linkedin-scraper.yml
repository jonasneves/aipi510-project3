name: LinkedIn Salary Data Collection

on:
  # Run daily at 1 AM UTC (before ML pipeline at 2 AM)
  schedule:
    - cron: '0 1 * * *'

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      runners:
        description: 'Number of parallel runners (1-20)'
        type: choice
        options:
          - '10'
          - '5'
          - '15'
          - '20'
        default: '10'

permissions:
  id-token: write
  contents: read

jobs:
  # Job 1: Generate search query matrix
  generate-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      timestamp: ${{ steps.set-matrix.outputs.timestamp }}

    steps:
      - name: Generate search matrix
        id: set-matrix
        run: |
          TIMESTAMP=$(date -u +"%Y-%m-%dT%H-%M-%SZ")
          echo "timestamp=$TIMESTAMP" >> $GITHUB_OUTPUT

          # Create matrix with search query batches
          cat << 'EOF' > matrix.json
          {
            "include": [
              {
                "batch": "1",
                "queries": [
                  {"keyword": "machine learning engineer", "location": "United States", "salary": "100000", "limit": "100"},
                  {"keyword": "machine learning engineer", "location": "California", "salary": "120000", "limit": "100"}
                ]
              },
              {
                "batch": "2",
                "queries": [
                  {"keyword": "data scientist", "location": "United States", "salary": "100000", "limit": "100"},
                  {"keyword": "data scientist", "location": "New York", "salary": "120000", "limit": "100"}
                ]
              },
              {
                "batch": "3",
                "queries": [
                  {"keyword": "AI engineer", "location": "United States", "salary": "100000", "limit": "100"},
                  {"keyword": "AI engineer", "location": "Washington", "salary": "120000", "limit": "100"}
                ]
              },
              {
                "batch": "4",
                "queries": [
                  {"keyword": "deep learning engineer", "location": "United States", "salary": "120000", "limit": "100"},
                  {"keyword": "computer vision engineer", "location": "United States", "salary": "120000", "limit": "100"}
                ]
              },
              {
                "batch": "5",
                "queries": [
                  {"keyword": "NLP engineer", "location": "United States", "salary": "120000", "limit": "100"},
                  {"keyword": "research scientist", "location": "United States", "salary": "120000", "limit": "100"}
                ]
              },
              {
                "batch": "6",
                "queries": [
                  {"keyword": "ML ops engineer", "location": "United States", "salary": "120000", "limit": "100"},
                  {"keyword": "AI researcher", "location": "United States", "salary": "120000", "limit": "100"}
                ]
              },
              {
                "batch": "7",
                "queries": [
                  {"keyword": "machine learning", "location": "San Francisco", "salary": "150000", "limit": "100"},
                  {"keyword": "machine learning", "location": "Seattle", "salary": "140000", "limit": "100"}
                ]
              },
              {
                "batch": "8",
                "queries": [
                  {"keyword": "data science", "location": "San Francisco", "salary": "150000", "limit": "100"},
                  {"keyword": "data science", "location": "Boston", "salary": "130000", "limit": "100"}
                ]
              },
              {
                "batch": "9",
                "queries": [
                  {"keyword": "artificial intelligence", "location": "United States", "salary": "120000", "limit": "100"},
                  {"keyword": "machine learning intern", "location": "United States", "salary": "40000", "limit": "50"}
                ]
              },
              {
                "batch": "10",
                "queries": [
                  {"keyword": "data scientist intern", "location": "United States", "salary": "40000", "limit": "50"},
                  {"keyword": "AI intern", "location": "United States", "salary": "40000", "limit": "50"}
                ]
              }
            ]
          }
          EOF

          echo "matrix=$(cat matrix.json | jq -c .)" >> $GITHUB_OUTPUT

  # Job 2: Parallel data collection
  collect-data:
    needs: generate-matrix
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 20
      fail-fast: false
      matrix: ${{ fromJson(needs.generate-matrix.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: linkedin-scraper/package.json

      - name: Install dependencies
        working-directory: linkedin-scraper
        run: npm ci

      - name: Collect data for batch ${{ matrix.batch }}
        working-directory: linkedin-scraper
        run: |
          # Create custom collection script for this batch
          cat << 'EOF' > scripts/collect-batch.js
          const fs = require("fs");
          const path = require("path");
          const linkedIn = require("../index");

          const OUTPUT_DIR = "./data";
          const BATCH_ID = process.env.BATCH_ID;
          const TIMESTAMP = process.env.TIMESTAMP;
          const OUTPUT_FILE = path.join(OUTPUT_DIR, `batch-${BATCH_ID}-${TIMESTAMP}.jsonl`);

          if (!fs.existsSync(OUTPUT_DIR)) {
            fs.mkdirSync(OUTPUT_DIR, { recursive: true });
          }

          const writeStream = fs.createWriteStream(OUTPUT_FILE, { flags: "a" });
          const queries = JSON.parse(process.env.QUERIES);
          const seenJobUrls = new Set();

          let totalJobs = 0;
          let jobsWithSalary = 0;

          function saveJob(job) {
            if (seenJobUrls.has(job.jobUrl)) return false;
            seenJobUrls.add(job.jobUrl);

            const jobData = {
              collected_at: new Date().toISOString(),
              position: job.position,
              company: job.company,
              location: job.location,
              posted_date: job.date,
              job_url: job.jobUrl,
              salary: job.salary,
              seniority_level: job.seniorityLevel,
              employment_type: job.employmentType,
              job_function: job.jobFunction,
              industries: job.industries,
              applicant_count: job.applicantCount,
              experience_years: job.experienceYears,
              education: job.education,
              skills: job.skills || [],
              company_logo: job.companyLogo,
              description: job.description ? job.description.substring(0, 500) : "",
            };

            writeStream.write(JSON.stringify(jobData) + "\n");
            totalJobs++;
            if (job.salary && job.salary !== "Not specified") jobsWithSalary++;
            return true;
          }

          async function collect() {
            console.log(`Batch ${BATCH_ID}: Processing ${queries.length} queries\n`);

            for (const [i, query] of queries.entries()) {
              console.log(`[${i+1}/${queries.length}] ${query.keyword} in ${query.location}`);

              try {
                const jobs = await linkedIn.query({
                  ...query,
                  dateSincePosted: "past Week",
                  sortBy: "recent",
                  fetchJobDetails: true,
                });

                let saved = 0;
                jobs.forEach(job => { if (saveJob(job)) saved++; });
                console.log(`  Saved ${saved} unique jobs`);

                if (i < queries.length - 1) {
                  await new Promise(r => setTimeout(r, 3000));
                }
              } catch (error) {
                console.error(`  Error: ${error.message}`);
              }
            }

            writeStream.end();
            console.log(`\nBatch ${BATCH_ID} complete: ${totalJobs} jobs (${jobsWithSalary} with salary)`);
          }

          collect().catch(e => { console.error(e); process.exit(1); });
          EOF

          # Run the batch collection
          BATCH_ID=${{ matrix.batch }} \
          TIMESTAMP=${{ needs.generate-matrix.outputs.timestamp }} \
          QUERIES='${{ toJson(matrix.queries) }}' \
          node scripts/collect-batch.js

        env:
          NODE_ENV: production

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Upload batch data to S3
        working-directory: linkedin-scraper
        run: |
          aws s3 sync ./data s3://ai-salary-predictor/linkedin/raw/ \
            --exclude "*" \
            --include "batch-${{ matrix.batch }}-*.jsonl" \
            --storage-class INTELLIGENT_TIERING

          echo "Uploaded batch ${{ matrix.batch }} to S3"

  # Job 3: Consolidate and deduplicate
  consolidate:
    needs: [generate-matrix, collect-data]
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: linkedin-scraper/package.json

      - name: Install dependencies
        working-directory: linkedin-scraper
        run: npm ci

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: us-east-1

      - name: Download all batch files from S3
        working-directory: linkedin-scraper
        run: |
          mkdir -p ./data
          aws s3 sync s3://ai-salary-predictor/linkedin/raw/ ./data/ \
            --exclude "*" \
            --include "batch-*-${{ needs.generate-matrix.outputs.timestamp }}.jsonl"

      - name: Deduplicate consolidated data
        working-directory: linkedin-scraper
        run: |
          node scripts/deduplicate-data.js \
            ./data \
            ./data/consolidated-${{ needs.generate-matrix.outputs.timestamp }}.jsonl

      - name: Upload consolidated file
        working-directory: linkedin-scraper
        run: |
          aws s3 cp \
            ./data/consolidated-${{ needs.generate-matrix.outputs.timestamp }}.jsonl \
            s3://ai-salary-predictor/linkedin/consolidated/ \
            --storage-class INTELLIGENT_TIERING

      - name: Create summary
        working-directory: linkedin-scraper
        run: |
          TIMESTAMP="${{ needs.generate-matrix.outputs.timestamp }}"
          TOTAL_SIZE=$(du -sh ./data | cut -f1)
          FILE_COUNT=$(ls -1 ./data/*.jsonl 2>/dev/null | wc -l)

          jq -n \
            --arg ts "$TIMESTAMP" \
            --arg size "$TOTAL_SIZE" \
            --arg files "$FILE_COUNT" \
            '{
              timestamp: $ts,
              total_size: $size,
              files_collected: $files,
              status: "success",
              parallel_runners: 10
            }' > ./data/run-summary.json

          aws s3 cp ./data/run-summary.json \
            s3://ai-salary-predictor/metadata/latest-run.json

          echo "## LinkedIn Data Collection Complete! âœ…" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $TIMESTAMP" >> $GITHUB_STEP_SUMMARY
          echo "**Total Size:** $TOTAL_SIZE" >> $GITHUB_STEP_SUMMARY
          echo "**Files Collected:** $FILE_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat ./data/run-summary.json >> $GITHUB_STEP_SUMMARY
