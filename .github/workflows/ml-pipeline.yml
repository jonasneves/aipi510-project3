name: ML Pipeline

on:
  push:
    branches: [main]
  pull_request:
  workflow_dispatch:
    inputs:
      force_collect:
        description: 'Force data collection from APIs (ignore S3 cache)'
        type: boolean
        default: false

env:
  PYTHON_VERSION: "3.11"
  S3_BUCKET: ai-salary-predictor
  AWS_REGION: us-east-1

# Required for OIDC authentication with AWS
permissions:
  id-token: write
  contents: read

jobs:
  collect:
    runs-on: ubuntu-latest
    outputs:
      data_source: ${{ steps.check-cache.outputs.source }}
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - run: pip install -r requirements.txt

      # Configure AWS credentials first to check S3 cache
      - name: Configure AWS credentials
        if: github.event_name != 'pull_request'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      # Try to pull cached data from S3
      - name: Check S3 cache
        id: check-cache
        run: |
          mkdir -p data/raw

          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "source=api" >> $GITHUB_OUTPUT
            echo "PR build - will collect from APIs"
          elif [ "${{ inputs.force_collect }}" = "true" ]; then
            echo "source=api" >> $GITHUB_OUTPUT
            echo "Force collect enabled - will collect from APIs"
          else
            # Try to download from S3
            if aws s3 sync s3://${{ env.S3_BUCKET }}/data/raw/ data/raw/ 2>/dev/null; then
              # Check if we got any data
              if [ "$(ls -A data/raw/*.parquet 2>/dev/null)" ]; then
                echo "source=s3" >> $GITHUB_OUTPUT
                echo "Using cached data from S3"
              else
                echo "source=api" >> $GITHUB_OUTPUT
                echo "S3 cache empty - will collect from APIs"
              fi
            else
              echo "source=api" >> $GITHUB_OUTPUT
              echo "S3 not available - will collect from APIs"
            fi
          fi

      # Only collect from APIs if S3 cache miss
      - name: Collect data from APIs
        if: steps.check-cache.outputs.source == 'api'
        env:
          BLS_API_KEY: ${{ secrets.BLS_API_KEY }}
          ADZUNA_APP_ID: ${{ secrets.ADZUNA_APP_ID }}
          ADZUNA_API_KEY: ${{ secrets.ADZUNA_API_KEY }}
        run: python -m src.main collect --source all

      - name: Summary
        run: |
          echo "## Data Collection" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Source:** ${{ steps.check-cache.outputs.source }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Source | Records |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|---------|" >> $GITHUB_STEP_SUMMARY
          for f in data/raw/*.parquet; do
            name=$(basename "$f" .parquet)
            count=$(python -c "import pandas as pd; print(len(pd.read_parquet('$f')))" 2>/dev/null || echo "0")
            echo "| $name | $count |" >> $GITHUB_STEP_SUMMARY
          done

      - uses: actions/upload-artifact@v4
        with:
          name: raw-data
          path: data/raw/
          retention-days: 1

      # Upload to S3 only if we collected fresh data
      - name: Upload raw data to S3
        if: github.event_name != 'pull_request' && steps.check-cache.outputs.source == 'api'
        run: |
          aws s3 sync data/raw/ s3://${{ env.S3_BUCKET }}/data/raw/ \
            --delete \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }}"
          echo "Raw data uploaded to s3://${{ env.S3_BUCKET }}/data/raw/"

  merge:
    needs: collect
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - run: pip install -r requirements.txt

      - uses: actions/download-artifact@v4
        with:
          name: raw-data
          path: data/raw/

      - name: Merge data
        run: python -m src.main merge

      - name: Summary
        run: |
          echo "## Data Merge" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          python -c "
          import pandas as pd
          df = pd.read_parquet('data/processed/merged_salary_data.parquet')
          print(f'**Total records:** {len(df)}')
          print(f'')
          print(f'**Salary range:** \${df[\"annual_salary\"].min():,.0f} - \${df[\"annual_salary\"].max():,.0f}')
          print(f'')
          print(f'**Median salary:** \${df[\"annual_salary\"].median():,.0f}')
          " >> $GITHUB_STEP_SUMMARY

      - uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: data/processed/
          retention-days: 1

      - name: Configure AWS credentials
        if: github.event_name != 'pull_request'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Upload processed data to S3
        if: github.event_name != 'pull_request'
        run: |
          aws s3 sync data/processed/ s3://${{ env.S3_BUCKET }}/data/processed/ \
            --delete \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }}"
          echo "Processed data uploaded to s3://${{ env.S3_BUCKET }}/data/processed/"

  train:
    needs: merge
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      - run: pip install -r requirements.txt

      - uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed/

      - name: Train model
        run: python -m src.main train 2>&1 | tee train.log

      - name: Summary
        run: |
          echo "## Model Training" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Metrics" >> $GITHUB_STEP_SUMMARY
          grep -A6 "Test Metrics:" train.log | tail -5 >> $GITHUB_STEP_SUMMARY || echo "No test metrics found"
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Top Features" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          grep -A15 "Top 15" train.log | tail -15 >> $GITHUB_STEP_SUMMARY || echo "No feature importance found"
          echo '```' >> $GITHUB_STEP_SUMMARY

      - uses: actions/upload-artifact@v4
        with:
          name: model
          path: models/
          retention-days: 7
          if-no-files-found: warn

      - name: Configure AWS credentials
        if: github.event_name != 'pull_request'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Upload model to S3
        if: github.event_name != 'pull_request'
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)

          aws s3 sync models/ s3://${{ env.S3_BUCKET }}/models/${TIMESTAMP}/ \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }}"

          aws s3 sync models/ s3://${{ env.S3_BUCKET }}/models/latest/ \
            --delete \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }},timestamp=${TIMESTAMP}"

          echo "Model uploaded to s3://${{ env.S3_BUCKET }}/models/latest/"
          echo "Model versioned at s3://${{ env.S3_BUCKET }}/models/${TIMESTAMP}/"
