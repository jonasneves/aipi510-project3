name: Pipeline & Deploy

# This workflow collects salary data, trains the ML model, and deploys the webapp + reports to GitHub Pages
#
# Data Collection Strategy:
#   - H1B: Can be forced or cached from S3 (updated annually)
#   - Adzuna: Can be forced or cached from S3 (API rate limits)
#   - LinkedIn: ALWAYS from S3 (has dedicated collection workflow)
#
# Training Profiles:
#   - all: Train on all available sources (default)
#   - h1b_only: Train only on government H1B data
#   - linkedin_only: Train only on LinkedIn data with rich features
#   - h1b_linkedin: Train on H1B + LinkedIn (highest quality sources)
#   - external_only: Train on external sources only (Adzuna + LinkedIn)

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'config.yaml'
      - 'requirements.txt'
      - 'frontend-react/**'
      - '.github/workflows/pipeline-and-deploy.yml'
  pull_request:
    paths:
      - 'src/**'
      - 'config.yaml'
      - 'requirements.txt'
      - 'frontend-react/**'
      - '.github/workflows/pipeline-and-deploy.yml'
  workflow_dispatch:
    inputs:
      training_profile:
        description: 'Training profile to use (determines which data sources to collect/train)'
        type: choice
        options:
          - all
          - h1b_only
          - linkedin_only
          - adzuna_only
        default: all
      force_refresh:
        description: 'Force data collection for ALL sources in selected profile (ignore S3 cache)'
        type: boolean
        default: false
      force_h1b:
        description: 'Force only H1B collection (only if profile includes H1B)'
        type: boolean
        default: false
      force_adzuna:
        description: 'Force only Adzuna collection (only if profile includes Adzuna)'
        type: boolean
        default: false

env:
  PYTHON_VERSION: "3.11"
  S3_BUCKET: ai-salary-predictor
  AWS_REGION: us-east-1

# Required for OIDC authentication with AWS
permissions:
  id-token: write
  contents: read

jobs:
  collect:
    runs-on: ubuntu-24.04-arm
    outputs:
      collect_sources: ${{ steps.check-cache.outputs.collect_sources }}
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      # Advanced pip caching
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ runner.arch }}-
            pip-${{ runner.os }}-

      - run: pip install -r requirements.txt

      # Configure AWS credentials first to check S3 cache
      - name: Configure AWS credentials
        if: github.event_name != 'pull_request'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      # Try to pull cached data from S3 (per-source control)
      - name: Check S3 cache
        id: check-cache
        run: |
          # Create hierarchical directory structure
          mkdir -p data/h1b/raw data/h1b/processed
          mkdir -p data/linkedin/raw data/linkedin/processed
          mkdir -p data/adzuna/raw data/adzuna/processed

          # Track which sources need collection
          collect_sources=""

          # Determine which sources are needed based on training profile
          PROFILE="${{ inputs.training_profile }}"
          NEEDS_H1B=false
          NEEDS_ADZUNA=false
          NEEDS_LINKEDIN=false

          case "$PROFILE" in
            "all")
              NEEDS_H1B=true
              NEEDS_ADZUNA=true
              NEEDS_LINKEDIN=true
              ;;
            "h1b_only")
              NEEDS_H1B=true
              ;;
            "linkedin_only")
              NEEDS_LINKEDIN=true
              ;;
            "h1b_linkedin")
              NEEDS_H1B=true
              NEEDS_LINKEDIN=true
              ;;
            "external_only")
              NEEDS_ADZUNA=true
              NEEDS_LINKEDIN=true
              ;;
            *)
              # Default to all if unknown profile
              NEEDS_H1B=true
              NEEDS_ADZUNA=true
              NEEDS_LINKEDIN=true
              ;;
          esac

          echo "Training profile: $PROFILE"
          echo "  Needs H1B: $NEEDS_H1B"
          echo "  Needs Adzuna: $NEEDS_ADZUNA"
          echo "  Needs LinkedIn: $NEEDS_LINKEDIN"

          # PR builds: collect H1B and Adzuna if needed by profile
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            [ "$NEEDS_H1B" = "true" ] && collect_sources="$collect_sources h1b"
            [ "$NEEDS_ADZUNA" = "true" ] && collect_sources="$collect_sources adzuna"
            echo "PR build - will collect needed sources: $collect_sources"
          else
            # Check H1B (only if needed by profile)
            if [ "$NEEDS_H1B" = "true" ]; then
              if [ "${{ inputs.force_refresh }}" = "true" ] || [ "${{ inputs.force_h1b }}" = "true" ]; then
                collect_sources="$collect_sources h1b"
                echo "Force H1B collection enabled"
              else
                # Try to download from S3
                if aws s3 sync s3://${{ env.S3_BUCKET }}/data/h1b/ data/h1b/ 2>/dev/null && \
                   [ -f "data/h1b/processed/h1b_ai_salaries.parquet" ]; then
                  echo "Using cached H1B data from S3"
                else
                  collect_sources="$collect_sources h1b"
                  echo "H1B S3 cache miss - will collect from APIs"
                fi
              fi
            else
              echo "H1B not needed for profile '$PROFILE' - skipping"
            fi

            # Check Adzuna (only if needed by profile)
            if [ "$NEEDS_ADZUNA" = "true" ]; then
              if [ "${{ inputs.force_refresh }}" = "true" ] || [ "${{ inputs.force_adzuna }}" = "true" ]; then
                collect_sources="$collect_sources adzuna"
                echo "Force Adzuna collection enabled"
              else
                # Try to download from S3
                if aws s3 sync s3://${{ env.S3_BUCKET }}/data/adzuna/processed/ data/adzuna/processed/ 2>/dev/null && \
                   [ -f "data/adzuna/processed/adzuna_jobs.parquet" ]; then
                  echo "Using cached Adzuna data from S3"
                else
                  collect_sources="$collect_sources adzuna"
                  echo "Adzuna S3 cache miss - will collect from APIs"
                fi
              fi
            else
              echo "Adzuna not needed for profile '$PROFILE' - skipping"
            fi
          fi

          # LinkedIn: ALWAYS from S3 (has separate workflow, only if needed by profile)
          if [ "$NEEDS_LINKEDIN" = "true" ]; then
            echo "LinkedIn: Downloading consolidated data from S3"

            # Download the comprehensive consolidated parquet file
            mkdir -p data/linkedin/processed
            aws s3 cp s3://${{ env.S3_BUCKET }}/data/linkedin/processed/linkedin_ai_jobs.parquet \
              data/linkedin/processed/linkedin_ai_jobs.parquet 2>/dev/null || \
              echo "  Warning: No consolidated LinkedIn data in S3"
          else
            echo "LinkedIn not needed for profile '$PROFILE' - skipping"
          fi

          # Set outputs
          if [ -n "$collect_sources" ]; then
            echo "collect_sources=$collect_sources" >> $GITHUB_OUTPUT
            echo "Will collect: $collect_sources"
          else
            echo "collect_sources=" >> $GITHUB_OUTPUT
            echo "All data from S3 cache"
          fi

      # Collect from APIs (only needed sources)
      - name: Collect data from APIs
        if: steps.check-cache.outputs.collect_sources != ''
        env:
          BLS_API_KEY: ${{ secrets.BLS_API_KEY }}
          ADZUNA_APP_ID: ${{ secrets.ADZUNA_APP_ID }}
          ADZUNA_API_KEY: ${{ secrets.ADZUNA_API_KEY }}
        run: |
          sources="${{ steps.check-cache.outputs.collect_sources }}"

          # Build command with only needed sources
          cmd="python -m src.main collect"
          for source in $sources; do
            if [ "$source" = "h1b" ]; then
              cmd="$cmd --source h1b"
            elif [ "$source" = "adzuna" ]; then
              cmd="$cmd --source jobs"
            fi
          done

          echo "Running: $cmd"
          $cmd

      # Fallback: Download missing files from S3 cache
      - name: Fallback to S3 cache for missing files
        if: steps.check-cache.outputs.collect_sources != '' && github.event_name != 'pull_request'
        run: |
          echo "Checking for missing data files..."

          # Check H1B files
          if [ ! -f "data/h1b/processed/h1b_ai_salaries.parquet" ]; then
            echo "  Missing: h1b_ai_salaries.parquet"
            aws s3 cp s3://${{ env.S3_BUCKET }}/data/h1b/processed/h1b_ai_salaries.parquet data/h1b/processed/ 2>/dev/null && \
              echo "  ✓ Recovered h1b_ai_salaries.parquet" || echo "  ✗ Could not recover"
          fi

          if [ ! -f "data/h1b/raw/h1b_lca_fy2023.xlsx" ]; then
            aws s3 cp s3://${{ env.S3_BUCKET }}/data/h1b/raw/h1b_lca_fy2023.xlsx data/h1b/raw/ 2>/dev/null && \
              echo "  ✓ Recovered h1b_lca_fy2023.xlsx" || echo "  ✗ Could not recover"
          fi

          if [ ! -f "data/h1b/raw/h1b_lca_fy2024.xlsx" ]; then
            aws s3 cp s3://${{ env.S3_BUCKET }}/data/h1b/raw/h1b_lca_fy2024.xlsx data/h1b/raw/ 2>/dev/null && \
              echo "  ✓ Recovered h1b_lca_fy2024.xlsx" || echo "  ✗ Could not recover"
          fi

          # Check LinkedIn file
          if [ ! -f "data/linkedin/processed/linkedin_ai_jobs.parquet" ]; then
            echo "  Missing: linkedin_ai_jobs.parquet"
            aws s3 cp s3://${{ env.S3_BUCKET }}/data/linkedin/processed/linkedin_ai_jobs.parquet data/linkedin/processed/ 2>/dev/null && \
              echo "  ✓ Recovered linkedin_ai_jobs.parquet" || echo "  ✗ Could not recover"
          fi

          # Check Adzuna file
          if [ ! -f "data/adzuna/processed/adzuna_jobs.parquet" ]; then
            echo "  Missing: adzuna_jobs.parquet"
            aws s3 cp s3://${{ env.S3_BUCKET }}/data/adzuna/processed/adzuna_jobs.parquet data/adzuna/processed/ 2>/dev/null && \
              echo "  ✓ Recovered adzuna_jobs.parquet" || echo "  ✗ Could not recover"
          fi

      - name: Summary
        run: |
          echo "## Data Collection" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          collected="${{ steps.check-cache.outputs.collect_sources }}"
          if [ -n "$collected" ]; then
            echo "**Collected from APIs:** $collected" >> $GITHUB_STEP_SUMMARY
          else
            echo "**All data from S3 cache**" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Source | Records | Origin |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|---------|--------|" >> $GITHUB_STEP_SUMMARY

          # Check H1B
          if [ -f "data/h1b/processed/h1b_ai_salaries.parquet" ]; then
            count=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/h1b/processed/h1b_ai_salaries.parquet')))" 2>/dev/null || echo "0")
            origin="s3"
            if echo "$collected" | grep -q "h1b"; then origin="api"; fi
            echo "| H1B | $count | $origin |" >> $GITHUB_STEP_SUMMARY
          fi

          # Check LinkedIn (always S3)
          if [ -f "data/linkedin/processed/linkedin_ai_jobs.parquet" ]; then
            count=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/linkedin/processed/linkedin_ai_jobs.parquet')))" 2>/dev/null || echo "0")
            echo "| LinkedIn | $count | s3 |" >> $GITHUB_STEP_SUMMARY
          fi

          # Check Adzuna
          if [ -f "data/adzuna/processed/adzuna_jobs.parquet" ]; then
            count=$(python -c "import pandas as pd; print(len(pd.read_parquet('data/adzuna/processed/adzuna_jobs.parquet')))" 2>/dev/null || echo "0")
            origin="s3"
            if echo "$collected" | grep -q "adzuna"; then origin="api"; fi
            echo "| Adzuna | $count | $origin |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Data Samples" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Show sample data from each source
          python -c "
          import pandas as pd
          import sys
          import json

          def show_sample(file_path, source_name):
              try:
                  df = pd.read_parquet(file_path)
                  if len(df) == 0:
                      return

                  # Get sample of 3 records
                  sample_df = df.head(3)

                  print(f'<details>')
                  print(f'<summary><b>{source_name}</b> (showing 3 of {len(df):,} records)</summary>')
                  print(f'')
                  print(f'\`\`\`json')

                  # Convert each record to JSON for clean display
                  for i, (_, row) in enumerate(sample_df.iterrows(), 1):
                      record = row.to_dict()
                      print(json.dumps(record, indent=2, default=str))
                      if i < len(sample_df):
                          print()

                  print(f'\`\`\`')
                  print(f'</details>')
                  print(f'')
              except Exception as e:
                  print(f'<!-- Error loading {source_name}: {str(e)} -->', file=sys.stderr)

          # Show samples from each source
          show_sample('data/h1b/processed/h1b_ai_salaries.parquet', 'H1B Data')
          show_sample('data/linkedin/processed/linkedin_ai_jobs.parquet', 'LinkedIn Data')
          show_sample('data/adzuna/processed/adzuna_jobs.parquet', 'Adzuna Data')
          " >> $GITHUB_STEP_SUMMARY 2>&1 || echo "Could not generate data samples" >> $GITHUB_STEP_SUMMARY

      - uses: actions/upload-artifact@v4
        with:
          name: collected-data
          path: data/
          retention-days: 1

      # Upload to S3 only if we collected fresh data
      - name: Upload collected data to S3
        if: github.event_name != 'pull_request' && steps.check-cache.outputs.collect_sources != ''
        run: |
          # Upload hierarchical structure
          aws s3 sync data/h1b/ s3://${{ env.S3_BUCKET }}/data/h1b/ \
            --delete \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }}"

          aws s3 sync data/linkedin/processed/ s3://${{ env.S3_BUCKET }}/data/linkedin/processed/ \
            --delete \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }}"

          aws s3 sync data/adzuna/processed/ s3://${{ env.S3_BUCKET }}/data/adzuna/processed/ \
            --delete \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }}"

          echo "Data uploaded to s3://${{ env.S3_BUCKET }}/data/"

  merge:
    needs: collect
    runs-on: ubuntu-24.04-arm
    outputs:
      data_source: ${{ steps.check-cache.outputs.source }}
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      # Advanced pip caching
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ runner.arch }}-
            pip-${{ runner.os }}-

      - run: pip install -r requirements.txt

      - name: Configure AWS credentials
        if: github.event_name != 'pull_request'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      # Always run merge to ensure latest data is used
      - name: Prepare merge
        id: check-cache
        run: |
          mkdir -p data/merged

          # Always run fresh merge (fast operation, ensures latest data)
          # This avoids stale cache issues when source data is updated
          echo "source=merge" >> $GITHUB_OUTPUT
          echo "Running fresh merge with latest data"

      - uses: actions/download-artifact@v4
        if: steps.check-cache.outputs.source == 'merge'
        with:
          name: collected-data
          path: data/

      - name: Merge data
        if: steps.check-cache.outputs.source == 'merge'
        run: |
          PROFILE="${{ inputs.training_profile }}"
          if [ -z "$PROFILE" ]; then
            PROFILE="all"
          fi
          echo "Using training profile: $PROFILE"
          python -m src.main merge --profile "$PROFILE"

      - name: Summary
        run: |
          echo "## Data Merge" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Source:** ${{ steps.check-cache.outputs.source }}" >> $GITHUB_STEP_SUMMARY

          PROFILE="${{ inputs.training_profile }}"
          if [ -n "$PROFILE" ] && [ "$PROFILE" != "all" ]; then
            echo "**Training Profile:** $PROFILE" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          python -c "
          import pandas as pd
          df = pd.read_parquet('data/merged/merged_salary_data.parquet')
          print(f'**Total records:** {len(df)}')
          print(f'')
          print(f'**Salary range:** \${df[\"annual_salary\"].min():,.0f} - \${df[\"annual_salary\"].max():,.0f}')
          print(f'')
          print(f'**Median salary:** \${df[\"annual_salary\"].median():,.0f}')
          " >> $GITHUB_STEP_SUMMARY

      - uses: actions/upload-artifact@v4
        with:
          name: merged-data
          path: data/merged/
          retention-days: 1

      - name: Upload merged data to S3
        if: github.event_name != 'pull_request' && steps.check-cache.outputs.source == 'merge'
        run: |
          aws s3 sync data/merged/ s3://${{ env.S3_BUCKET }}/data/merged/ \
            --delete \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }}"
          echo "Merged data uploaded to s3://${{ env.S3_BUCKET }}/data/merged/"

  eda:
    needs: merge
    runs-on: ubuntu-24.04-arm
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      # Advanced pip caching
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ runner.arch }}-
            pip-${{ runner.os }}-

      - run: pip install -r requirements.txt

      - uses: actions/download-artifact@v4
        with:
          name: merged-data
          path: data/merged/

      - name: Generate EDA Report
        run: python -m src.analysis.eda_report --keep-latest-only

      - name: Summary
        run: |
          echo "## EDA Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Status:** ✅ Generated" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Extract key stats from JSON
          if [ -f "docs/reports/eda_stats_"*.json ]; then
            python -c "
          import json
          import glob
          stats_file = sorted(glob.glob('docs/reports/eda_stats_*.json'))[-1]
          with open(stats_file) as f:
              stats = json.load(f)
          merged = stats.get('merged', {})
          sal = merged.get('salary', {})
          print(f'**Total records:** {merged.get(\"records\", 0):,}')
          print(f'')
          print(f'**Salary statistics:**')
          print(f'- Median: \${sal.get(\"median\", 0):,.0f}')
          print(f'- Mean: \${sal.get(\"mean\", 0):,.0f}')
          print(f'- Range: \${sal.get(\"min\", 0):,.0f} - \${sal.get(\"max\", 0):,.0f}')
          " >> $GITHUB_STEP_SUMMARY
          fi

      - uses: actions/upload-artifact@v4
        with:
          name: eda-reports
          path: docs/reports/
          retention-days: 30

      - name: Configure AWS credentials
        if: github.event_name != 'pull_request'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Upload EDA reports to S3
        if: github.event_name != 'pull_request'
        run: |
          # Upload only latest reports
          aws s3 sync docs/reports/ s3://${{ env.S3_BUCKET }}/eda/latest/ \
            --exclude "*" \
            --include "eda_report_latest.html" \
            --include "eda_stats_*.json" \
            --include "*_latest.png" \
            --include "salary_distributions_*.png" \
            --include "source_comparison_*.png" \
            --include "location_analysis_*.png" \
            --include "quality_metrics_*.png" \
            --include "skills_analysis_*.png" \
            --delete \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }}"

          echo "EDA reports uploaded to s3://${{ env.S3_BUCKET }}/eda/latest/"

  train:
    needs: merge
    runs-on: ubuntu-24.04-arm
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip

      # Advanced pip caching
      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ runner.arch }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ runner.arch }}-
            pip-${{ runner.os }}-

      - run: pip install -r requirements.txt

      - uses: actions/download-artifact@v4
        with:
          name: merged-data
          path: data/merged/

      - name: Train model
        run: |
          PROFILE="${{ inputs.training_profile }}"
          if [ -z "$PROFILE" ]; then
            PROFILE="all"
          fi
          echo "Using training profile: $PROFILE"
          python -m src.main train --profile "$PROFILE" 2>&1 | tee train.log
        continue-on-error: true
        id: train

      - name: Summary
        if: always()
        run: |
          echo "## Model Training" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          PROFILE="${{ inputs.training_profile }}"
          if [ -n "$PROFILE" ] && [ "$PROFILE" != "all" ]; then
            echo "**Training Profile:** $PROFILE" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          # Show training status
          if [ "${{ steps.train.outcome }}" == "success" ]; then
            echo "**Status:** ✅ Success" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Status:** ❌ Failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Error Log (last 50 lines)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -50 train.log >> $GITHUB_STEP_SUMMARY || echo "No log available"
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Metrics" >> $GITHUB_STEP_SUMMARY
          grep -A6 "Test Metrics:" train.log | tail -5 >> $GITHUB_STEP_SUMMARY || echo "No test metrics found"
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Top Features" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          grep -A15 "Top 15" train.log | tail -15 >> $GITHUB_STEP_SUMMARY || echo "No feature importance found"
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Generate MLflow report
        if: steps.train.outcome == 'success'
        run: |
          python scripts/generate_mlflow_report.py \
            --mlruns mlruns \
            --experiment salary_prediction \
            --model-dir models \
            --train-log train.log \
            --output docs/mlflow

      - uses: actions/upload-artifact@v4
        with:
          name: model
          path: models/
          retention-days: 7
          if-no-files-found: warn

      - uses: actions/upload-artifact@v4
        if: steps.train.outcome == 'success'
        with:
          name: mlflow-report
          path: docs/mlflow/
          retention-days: 7
          if-no-files-found: warn

      - name: Configure AWS credentials
        if: github.event_name != 'pull_request'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Upload model to S3
        if: github.event_name != 'pull_request'
        run: |
          # Only upload if models directory exists (training succeeded)
          if [ ! -d "models/" ] || [ -z "$(ls -A models/)" ]; then
            echo "No models found to upload. Training may have failed."
            exit 0
          fi

          TIMESTAMP=$(date +%Y%m%d_%H%M%S)

          aws s3 sync models/ s3://${{ env.S3_BUCKET }}/models/${TIMESTAMP}/ \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }}"

          aws s3 sync models/ s3://${{ env.S3_BUCKET }}/models/latest/ \
            --delete \
            --metadata "pipeline-run=${{ github.run_id }},commit=${{ github.sha }},timestamp=${TIMESTAMP}"

          echo "Model uploaded to s3://${{ env.S3_BUCKET }}/models/latest/"
          echo "Model versioned at s3://${{ env.S3_BUCKET }}/models/${TIMESTAMP}/"

  deploy-pages:
    needs: [eda, train]  # Note: eda and train run in parallel after merge
    if: ${{ github.event_name != 'pull_request' && always() }}
    runs-on: ubuntu-24.04-arm
    permissions:
      contents: write  # Required to push to gh-pages branch
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend-react/package-lock.json

      - name: Build webapp
        working-directory: frontend-react
        run: |
          npm ci
          npm run build

      - name: Download EDA reports
        if: needs.eda.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: eda-reports
          path: site/eda

      - name: Download MLflow report
        if: needs.train.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: mlflow-report
          path: site/mlflow

      - name: Prepare site content
        run: |
          rm -rf site_root
          mkdir -p site_root

          # Copy webapp to root
          if [ -d frontend-react/dist ]; then
            echo "Copying webapp to root..."
            cp -R frontend-react/dist/. site_root/
          fi

          # Copy reports to /reports subdirectory
          if [ -d site/eda ]; then
            mkdir -p site_root/reports
            if [ -d site/eda/docs/reports ]; then
              cp -R site/eda/docs/reports/. site_root/reports/
            else
              cp -R site/eda/. site_root/reports/
            fi
          fi

          if [ ! -d site_root/reports ] && [ -d docs/reports ]; then
            echo "Using repo docs/reports as fallback"
            mkdir -p site_root/reports
            cp -R docs/reports/. site_root/reports/
          fi

          if [ -d site_root/reports ]; then
            if [ -f site_root/reports/eda_report_latest.html ]; then
              mv site_root/reports/eda_report_latest.html site_root/reports/index.html
            fi
          fi

          # Copy MLflow reports to /mlflow subdirectory
          if [ -d site/mlflow ]; then
            mkdir -p site_root/mlflow
            if [ -d site/mlflow/docs/mlflow ]; then
              cp -R site/mlflow/docs/mlflow/. site_root/mlflow/
            else
              cp -R site/mlflow/. site_root/mlflow/
            fi
          fi

          if [ ! -d site_root/mlflow ] && [ -d docs/mlflow ]; then
            echo "Using repo docs/mlflow as fallback"
            mkdir -p site_root/mlflow
            cp -R docs/mlflow/. site_root/mlflow/
          fi

          echo "Publishing contents:"
          find site_root -maxdepth 2 -type f | sort

      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./site_root
          publish_branch: gh-pages
